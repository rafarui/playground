{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1 (10 Points)\n",
    "**List as many use cases for the dataset as possible.**\n",
    "1. Group cars with similar features to display together on the website.\n",
    "2. `Price` Prediction\n",
    "3. `Normalized-losses` prediction. We can see from the sample data thare are many NaN values for this fields. \n",
    "4. `symboling` classification. \n",
    "4. Create embedings to each car to be used in other models that use the car as a feature.\n",
    "5. Denoizer to fill the NaN values to improve data quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2 (10 Points)\n",
    "**Auto1 has a similar dataset (yet much larger...) \n",
    "Pick one of the use cases you listed in question 1 and describe how building a statistical model based on the dataset could best be used to improve Auto1â€™s business.**\n",
    "\n",
    "I chose the price prediction. I believe this is one of the most import prediction Auto1 has.  Since Auto1 has a similar dataset but much bigger the one for this test, I would suggest using an ensemble of different models, from linear models like linear regressions to non-linear, tree-based methods and  Neural networks. With this approach, we can extract the maximum of information from the limited data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3 (20 Points)\n",
    "**Implement the model you described in question 2 in R or Python. The code has to retrieve the data, train and test a statistical model, and report relevant performance criteria. **\n",
    "\n",
    "Check below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 (60 Points)\n",
    "**A. Explain each and every of your design choices (e.g., preprocessing, model selection, hyper parameters, evaluation criteria). Compare and contrast your choices with alternative methodologies.** \n",
    "Here I will give a summary of the idea. For more insights, please refer to the code below. \n",
    "\n",
    "1. preprocessing: Since we have limited data, for both, number of samples and num. of features, I did a small data preparation:\n",
    "   1. removed Nan-values from the target `price`, created categorical variables and a binary encoding for those. \n",
    "   1. renamed the columns with a slash.\n",
    "   1. shuffled the data frame to avoid any bias in the model.\n",
    "    \n",
    "1. model selection: Stacking (also called meta ensembling) is a model ensembling technique used to combine information from multiple predictive models to generate a new model. Stacking is most effective when the base models are significantly different. To create different models I used two different methods, lightgbm (gradient boosting framework) and Ordinary Least Squares (OLS).  For each method (lightgbm and OLS) I run 15 times changing the features used.   Due to the lack of samples, I chose to use a k-fold validation with `k=5` and a holdout of 10% of the samples for testing. I noticed a lot of variance between each fold (due to the lack of sample). I should use more folds to overcome this. Although, I am using ensembles, so I expect that this variance will be minimized.\n",
    "\n",
    "1. hyperparameters: For lightgbm, I tried to use small trees and col samples to increase the model diversity. I have not performed any parameter tuning, Usually, I would use `skopt` (https://scikit-optimize.github.io/), but due to time limitation, I use some standard values. I have run the lightgbm model with early stopping to get some intuition of the number of boosting iterations before the final version. But, again, the variance between folds was very large, so I decided to keep the early stopping for the final model. \n",
    "\n",
    "1. evaluation criteria: I used root mean square error for evaluation and loss function.\n",
    "\n",
    "\n",
    "**B. Describe how you would improve the model in Question 3 if you had more time.**\n",
    "\n",
    "1. since we have very few samples, I would change the 5-fold to a leave-one-out cross-validation. \n",
    "2. find optimal parameters for the lightgbm models\n",
    "3. try to fill NaN values in the dataset.\n",
    "4. More types of encodings for the categorical variables to increase the diversity of models. \n",
    "5. use xgboost\n",
    "6. use lasso and ridge regression, then I need to normalize the data due to their regularization. \n",
    "7. change the final staking model since it was not better than the average of the models. \n",
    "8. feature engineering for the dataset and the final stack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "% matplotlib inline\n",
    "pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filename = 'data.csv'\n",
    "dtypes = {'symboling': int,'normalized-losses': np.float, 'make': str,\n",
    "          'fuel-type': str, 'aspiration': str, 'num-of-doors': str,\n",
    "          'body-style': str, 'drive-wheels': str, 'engine-location': str, \n",
    "          'wheel-base': np.float32, 'length': np.float32,'width': np.float32,\n",
    "          'height': np.float32, 'curb-weight': np.float32, 'engine-type': str,\n",
    "          'num-of-cylinders': str, 'engine-size': np.float32, \n",
    "          'fuel-system': str, 'bore': np.float32, 'stroke': np.float32,\n",
    "          'compression-ratio': np.float32,  'horsepower': np.float32, \n",
    "          'peak-rpm': np.float32, 'city-mpg': np.float32,\n",
    "          'highway-mpg': np.float32, 'price': np.float32,\n",
    "         }\n",
    "\n",
    "df = pd.read_csv(filename, na_values='?',dtype=dtypes)\n",
    "df =  df.sample(frac=1, random_state=42)\n",
    "df.dropna(axis=0, subset=['price'], inplace=True)  #drop NaN since it is the target value.\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "y = df['price']\n",
    "df.drop(columns=['price'], inplace=True)\n",
    "df.columns = [i.replace(\"-\", \"_\")for i in df.columns]  #change the columns names to be able to use with OLS.\n",
    "\n",
    "categorical_feature = [col for col in df.columns \n",
    "                       if df.dtypes[col] == object]\n",
    "\n",
    "not_categorical = [col for col in df.columns \n",
    "                   if col not in categorical_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will transform the categorical variables to type category in pandas and also create a binary encoding for each categorical variable.\n",
    "OBS: we are going to use lightgbm, it can handle categorical variables, but I also chose to have binary encoded variables.\n",
    "We will use it to add some variety of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_encoded_cols = []\n",
    "for col in categorical_feature:\n",
    "    uniques = pd.DataFrame(list(df[col].unique()), columns = [col])\n",
    "    enc = ce.BinaryEncoder(verbose=1, cols=[col])\n",
    "    uniques = pd.concat([uniques,enc.fit_transform(uniques)], axis=1)\n",
    "    df= df.merge(uniques, how = 'left', on = col )\n",
    "    bin_encoded_cols.extend(list(uniques.columns[1:]))\n",
    "    df[col] = df[col].astype('category')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10% of the data to test. \n",
    "X_train, X_test, y_train, y_test = train_test_split( df, y, test_size=0.10, random_state=42)\n",
    "X_train.reset_index(drop=True,inplace=True)\n",
    "X_test.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main function to train the lightgbm models\n",
    "def train_model_LGBM(data_, y_ ,test_, folds_,feats = None, seed= 42, params_lgbm=None ):\n",
    "    if not params_lgbm:\n",
    "        params_lgbm = {\n",
    "                        'learning_rate': 0.01,\n",
    "                        'task': 'train',\n",
    "                        'boosting_type': 'gbdt',\n",
    "                        'objective': 'regression_l2',\n",
    "                        'metric': {'rmse'},\n",
    "                        'num_leaves': 50,\n",
    "                        'max_depth': 5,\n",
    "                        'feature_fraction' :.8,\n",
    "                        'seed':seed\n",
    "                        }\n",
    "\n",
    "    oof_preds = np.zeros(data_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    \n",
    "    if not feats:\n",
    "        feats = list(data_.columns)\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_, y_)):\n",
    "        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n",
    "        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n",
    "        \n",
    "        d_train = lgb.Dataset(trn_x, trn_y.values)\n",
    "        d_test = lgb.Dataset(val_x, val_y.values, reference = d_train)\n",
    "\n",
    "        bst = lgb.train(params_lgbm, d_train, 2000, valid_sets = [d_train,d_test], valid_names =['train','test'],\n",
    "                early_stopping_rounds=50, verbose_eval=1000)\n",
    "        \n",
    "        \n",
    "\n",
    "        oof_preds[val_idx] = bst.predict(val_x, num_iteration=bst.best_iteration)\n",
    "        sub_preds += bst.predict(test_[feats],\n",
    "                                 num_iteration=bst.best_iteration) / folds_.n_splits\n",
    "\n",
    "        print('Fold %2d MSE : %.6f' %\n",
    "              (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "\n",
    "\n",
    "    print('Full MSE score %.6f' % np.sqrt(mean_squared_error(y_, oof_preds)))\n",
    "\n",
    "    return oof_preds, sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the folds must be the same for all the models (OLS and lightgbm)\n",
    "kf = KFold(n_splits=5)\n",
    "num_models = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1243.83\ttest's rmse: 2362.63\n",
      "Early stopping, best iteration is:\n",
      "[1804]\ttrain's rmse: 964.934\ttest's rmse: 2281.4\n",
      "Fold  1 MSE : 2239.020406\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1175.98\ttest's rmse: 2728.8\n",
      "[2000]\ttrain's rmse: 852.471\ttest's rmse: 2647.4\n",
      "Fold  2 MSE : 2309.979774\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[912]\ttrain's rmse: 1472\ttest's rmse: 1891.93\n",
      "Fold  3 MSE : 1922.413091\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1338.63\ttest's rmse: 2584.88\n",
      "Early stopping, best iteration is:\n",
      "[1391]\ttrain's rmse: 1155.83\ttest's rmse: 2502.17\n",
      "Fold  4 MSE : 2713.261496\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttrain's rmse: 2424.37\ttest's rmse: 2166.66\n",
      "Fold  5 MSE : 2166.333497\n",
      "Full MSE score 2284.717917\n",
      "model 2\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1689.88\ttest's rmse: 3381.88\n",
      "[2000]\ttrain's rmse: 1063.16\ttest's rmse: 3083.83\n",
      "Fold  1 MSE : 3083.834713\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1511.57\ttest's rmse: 2653.62\n",
      "Early stopping, best iteration is:\n",
      "[1015]\ttrain's rmse: 1496.82\ttest's rmse: 2647.32\n",
      "Fold  2 MSE : 2647.322125\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[820]\ttrain's rmse: 2011.86\ttest's rmse: 2070.59\n",
      "Fold  3 MSE : 2070.588663\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1853.5\ttest's rmse: 3578.5\n",
      "[2000]\ttrain's rmse: 1218.4\ttest's rmse: 3184.44\n",
      "Fold  4 MSE : 3184.440943\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[265]\ttrain's rmse: 3139.2\ttest's rmse: 3524.68\n",
      "Fold  5 MSE : 3524.680757\n",
      "Full MSE score 2945.156259\n",
      "model 3\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1245.18\ttest's rmse: 2360.58\n",
      "Early stopping, best iteration is:\n",
      "[1568]\ttrain's rmse: 1025.99\ttest's rmse: 2287.52\n",
      "Fold  1 MSE : 2250.272714\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1177.97\ttest's rmse: 2722.01\n",
      "Early stopping, best iteration is:\n",
      "[1828]\ttrain's rmse: 892.624\ttest's rmse: 2659.55\n",
      "Fold  2 MSE : 2351.044165\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1405.55\ttest's rmse: 1879.1\n",
      "Early stopping, best iteration is:\n",
      "[975]\ttrain's rmse: 1424.75\ttest's rmse: 1875.14\n",
      "Fold  3 MSE : 1890.704746\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1332.67\ttest's rmse: 2586.85\n",
      "Early stopping, best iteration is:\n",
      "[1657]\ttrain's rmse: 1051.43\ttest's rmse: 2500.75\n",
      "Fold  4 MSE : 2720.194050\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttrain's rmse: 2431.15\ttest's rmse: 2166.67\n",
      "Fold  5 MSE : 2166.330377\n",
      "Full MSE score 2291.651953\n",
      "model 4\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1246.39\ttest's rmse: 2345.52\n",
      "Early stopping, best iteration is:\n",
      "[1632]\ttrain's rmse: 1010.35\ttest's rmse: 2275.71\n",
      "Fold  1 MSE : 2237.550106\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1178.35\ttest's rmse: 2732.11\n",
      "Early stopping, best iteration is:\n",
      "[1527]\ttrain's rmse: 980.09\ttest's rmse: 2676.77\n",
      "Fold  2 MSE : 2383.387090\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1401.75\ttest's rmse: 1898.92\n",
      "Early stopping, best iteration is:\n",
      "[1045]\ttrain's rmse: 1369.37\ttest's rmse: 1889.28\n",
      "Fold  3 MSE : 1898.914344\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1335.2\ttest's rmse: 2580.28\n",
      "Early stopping, best iteration is:\n",
      "[1495]\ttrain's rmse: 1107\ttest's rmse: 2482.16\n",
      "Fold  4 MSE : 2681.150876\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttrain's rmse: 2431.64\ttest's rmse: 2171.54\n",
      "Fold  5 MSE : 2171.312849\n",
      "Full MSE score 2288.939042\n",
      "model 5\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1247.8\ttest's rmse: 2351.83\n",
      "Early stopping, best iteration is:\n",
      "[1815]\ttrain's rmse: 961.4\ttest's rmse: 2276.27\n",
      "Fold  1 MSE : 2233.720831\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1181.11\ttest's rmse: 2708.03\n",
      "Early stopping, best iteration is:\n",
      "[1808]\ttrain's rmse: 900.913\ttest's rmse: 2628.16\n",
      "Fold  2 MSE : 2319.596431\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1402.01\ttest's rmse: 1894.41\n",
      "Early stopping, best iteration is:\n",
      "[1057]\ttrain's rmse: 1361.8\ttest's rmse: 1884.92\n",
      "Fold  3 MSE : 1892.947751\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1335.65\ttest's rmse: 2574.12\n",
      "Early stopping, best iteration is:\n",
      "[1611]\ttrain's rmse: 1071.15\ttest's rmse: 2475.25\n",
      "Fold  4 MSE : 2688.228379\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttrain's rmse: 2456.91\ttest's rmse: 2183.52\n",
      "Fold  5 MSE : 2183.247663\n",
      "Full MSE score 2278.000427\n",
      "model 6\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1688.6\ttest's rmse: 3379.26\n",
      "[2000]\ttrain's rmse: 1062.1\ttest's rmse: 3080.53\n",
      "Fold  1 MSE : 3080.530469\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1521.36\ttest's rmse: 2656.25\n",
      "Early stopping, best iteration is:\n",
      "[1005]\ttrain's rmse: 1516.41\ttest's rmse: 2654.41\n",
      "Fold  2 MSE : 2654.409525\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[904]\ttrain's rmse: 1898.35\ttest's rmse: 2075.9\n",
      "Fold  3 MSE : 2075.902705\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1848.19\ttest's rmse: 3562.51\n",
      "[2000]\ttrain's rmse: 1215.56\ttest's rmse: 3144.44\n",
      "Fold  4 MSE : 3144.440542\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[280]\ttrain's rmse: 3094.6\ttest's rmse: 3553.3\n",
      "Fold  5 MSE : 3553.298801\n",
      "Full MSE score 2944.770572\n",
      "model 7\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1248.35\ttest's rmse: 2351.91\n",
      "Early stopping, best iteration is:\n",
      "[1585]\ttrain's rmse: 1024.7\ttest's rmse: 2286.42\n",
      "Fold  1 MSE : 2258.767568\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1183.76\ttest's rmse: 2719.18\n",
      "Early stopping, best iteration is:\n",
      "[1753]\ttrain's rmse: 912.953\ttest's rmse: 2640.37\n",
      "Fold  2 MSE : 2345.403555\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1397.07\ttest's rmse: 1873.31\n",
      "Early stopping, best iteration is:\n",
      "[1135]\ttrain's rmse: 1304.32\ttest's rmse: 1852.79\n",
      "Fold  3 MSE : 1862.322491\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1335.9\ttest's rmse: 2596.12\n",
      "Early stopping, best iteration is:\n",
      "[1705]\ttrain's rmse: 1035.57\ttest's rmse: 2465.96\n",
      "Fold  4 MSE : 2683.375217\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttrain's rmse: 2425.54\ttest's rmse: 2162.14\n",
      "Fold  5 MSE : 2161.737193\n",
      "Full MSE score 2277.929063\n",
      "model 8\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1247.03\ttest's rmse: 2342.15\n",
      "Early stopping, best iteration is:\n",
      "[1527]\ttrain's rmse: 1045.16\ttest's rmse: 2279.63\n",
      "Fold  1 MSE : 2248.563369\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1187.1\ttest's rmse: 2699.01\n",
      "Early stopping, best iteration is:\n",
      "[1300]\ttrain's rmse: 1041.41\ttest's rmse: 2647.91\n",
      "Fold  2 MSE : 2385.515465\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1404.81\ttest's rmse: 1887.85\n",
      "Early stopping, best iteration is:\n",
      "[1006]\ttrain's rmse: 1400.17\ttest's rmse: 1884.78\n",
      "Fold  3 MSE : 1895.354155\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1341.23\ttest's rmse: 2578.11\n",
      "Early stopping, best iteration is:\n",
      "[1755]\ttrain's rmse: 1028.44\ttest's rmse: 2469.47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  4 MSE : 2693.956982\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[285]\ttrain's rmse: 2419.25\ttest's rmse: 2169.25\n",
      "Fold  5 MSE : 2169.097999\n",
      "Full MSE score 2293.533474\n",
      "model 9\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1250.47\ttest's rmse: 2343.38\n",
      "Early stopping, best iteration is:\n",
      "[1822]\ttrain's rmse: 954.388\ttest's rmse: 2259.92\n",
      "Fold  1 MSE : 2224.420324\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1180.8\ttest's rmse: 2739.18\n",
      "Early stopping, best iteration is:\n",
      "[1434]\ttrain's rmse: 1003.61\ttest's rmse: 2697.1\n",
      "Fold  2 MSE : 2411.424318\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[925]\ttrain's rmse: 1462.96\ttest's rmse: 1892.89\n",
      "Fold  3 MSE : 1908.011759\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1336.74\ttest's rmse: 2579.08\n",
      "Early stopping, best iteration is:\n",
      "[1528]\ttrain's rmse: 1104.85\ttest's rmse: 2481.58\n",
      "Fold  4 MSE : 2687.413040\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[284]\ttrain's rmse: 2429.15\ttest's rmse: 2174.13\n",
      "Fold  5 MSE : 2173.641306\n",
      "Full MSE score 2295.666669\n",
      "model 10\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1693.58\ttest's rmse: 3376.42\n",
      "[2000]\ttrain's rmse: 1065.96\ttest's rmse: 3066.31\n",
      "Fold  1 MSE : 3066.305155\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1515.97\ttest's rmse: 2688.36\n",
      "Early stopping, best iteration is:\n",
      "[997]\ttrain's rmse: 1519.26\ttest's rmse: 2684.5\n",
      "Fold  2 MSE : 2684.496751\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[862]\ttrain's rmse: 1961.22\ttest's rmse: 2092.09\n",
      "Fold  3 MSE : 2092.089208\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[175]\ttrain's rmse: 3774.75\ttest's rmse: 5068.98\n",
      "Fold  4 MSE : 5068.977713\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[271]\ttrain's rmse: 3122.11\ttest's rmse: 3542.08\n",
      "Fold  5 MSE : 3542.081599\n",
      "Full MSE score 3441.699129\n",
      "model 11\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1245.48\ttest's rmse: 2367.6\n",
      "Early stopping, best iteration is:\n",
      "[1904]\ttrain's rmse: 931.252\ttest's rmse: 2283.06\n",
      "Fold  1 MSE : 2234.009944\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1180.87\ttest's rmse: 2693.26\n",
      "Early stopping, best iteration is:\n",
      "[1865]\ttrain's rmse: 889.169\ttest's rmse: 2617.12\n",
      "Fold  2 MSE : 2317.346867\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[920]\ttrain's rmse: 1466.48\ttest's rmse: 1875.62\n",
      "Fold  3 MSE : 1893.892476\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1336.14\ttest's rmse: 2576.55\n",
      "Early stopping, best iteration is:\n",
      "[1543]\ttrain's rmse: 1098.89\ttest's rmse: 2493.87\n",
      "Fold  4 MSE : 2696.057669\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[288]\ttrain's rmse: 2419.1\ttest's rmse: 2179.43\n",
      "Fold  5 MSE : 2179.208483\n",
      "Full MSE score 2278.833132\n",
      "model 12\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1245\ttest's rmse: 2363.41\n",
      "Early stopping, best iteration is:\n",
      "[1620]\ttrain's rmse: 1013.45\ttest's rmse: 2303\n",
      "Fold  1 MSE : 2266.311517\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1183.37\ttest's rmse: 2691.19\n",
      "[2000]\ttrain's rmse: 864.492\ttest's rmse: 2609.25\n",
      "Fold  2 MSE : 2304.627492\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1403.72\ttest's rmse: 1869.45\n",
      "Early stopping, best iteration is:\n",
      "[1081]\ttrain's rmse: 1345.3\ttest's rmse: 1860.59\n",
      "Fold  3 MSE : 1871.800590\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1335.08\ttest's rmse: 2549.13\n",
      "Early stopping, best iteration is:\n",
      "[1350]\ttrain's rmse: 1168.6\ttest's rmse: 2459.77\n",
      "Fold  4 MSE : 2649.467780\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[290]\ttrain's rmse: 2401.86\ttest's rmse: 2162.52\n",
      "Fold  5 MSE : 2162.432981\n",
      "Full MSE score 2264.813861\n",
      "model 13\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1249.9\ttest's rmse: 2363.32\n",
      "Early stopping, best iteration is:\n",
      "[1759]\ttrain's rmse: 977.793\ttest's rmse: 2290.06\n",
      "Fold  1 MSE : 2249.984661\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1181.95\ttest's rmse: 2724.74\n",
      "Early stopping, best iteration is:\n",
      "[1560]\ttrain's rmse: 956.684\ttest's rmse: 2658.79\n",
      "Fold  2 MSE : 2363.793204\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1408.51\ttest's rmse: 1873.81\n",
      "Early stopping, best iteration is:\n",
      "[1134]\ttrain's rmse: 1313.46\ttest's rmse: 1855.24\n",
      "Fold  3 MSE : 1866.716986\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1341.15\ttest's rmse: 2573.56\n",
      "Early stopping, best iteration is:\n",
      "[1654]\ttrain's rmse: 1053.52\ttest's rmse: 2452.61\n",
      "Fold  4 MSE : 2660.373555\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[282]\ttrain's rmse: 2434.59\ttest's rmse: 2150.94\n",
      "Fold  5 MSE : 2150.617492\n",
      "Full MSE score 2273.205870\n",
      "model 14\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1248.47\ttest's rmse: 2342.24\n",
      "Early stopping, best iteration is:\n",
      "[1795]\ttrain's rmse: 968.054\ttest's rmse: 2264.11\n",
      "Fold  1 MSE : 2220.749908\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1180.14\ttest's rmse: 2714.76\n",
      "Early stopping, best iteration is:\n",
      "[1882]\ttrain's rmse: 883.509\ttest's rmse: 2635.03\n",
      "Fold  2 MSE : 2318.109875\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1399.79\ttest's rmse: 1889.27\n",
      "Early stopping, best iteration is:\n",
      "[960]\ttrain's rmse: 1429.97\ttest's rmse: 1883.94\n",
      "Fold  3 MSE : 1898.733285\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1333.88\ttest's rmse: 2572.14\n",
      "Early stopping, best iteration is:\n",
      "[1419]\ttrain's rmse: 1138.23\ttest's rmse: 2484.93\n",
      "Fold  4 MSE : 2678.953673\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttrain's rmse: 2434.05\ttest's rmse: 2172\n",
      "Fold  5 MSE : 2171.649417\n",
      "Full MSE score 2271.713257\n",
      "model 15\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1690.05\ttest's rmse: 3395.35\n",
      "[2000]\ttrain's rmse: 1061.26\ttest's rmse: 3105.63\n",
      "Fold  1 MSE : 3105.625969\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[1000]\ttrain's rmse: 1515.25\ttest's rmse: 2669.09\n",
      "Early stopping, best iteration is:\n",
      "[1038]\ttrain's rmse: 1476.98\ttest's rmse: 2665.77\n",
      "Fold  2 MSE : 2665.770964\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[870]\ttrain's rmse: 1950.44\ttest's rmse: 2077.21\n",
      "Fold  3 MSE : 2077.209104\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[173]\ttrain's rmse: 3788.3\ttest's rmse: 5063.38\n",
      "Fold  4 MSE : 5063.384066\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[274]\ttrain's rmse: 3091.57\ttest's rmse: 3537.82\n",
      "Fold  5 MSE : 3537.817759\n",
      "Full MSE score 3441.512977\n"
     ]
    }
   ],
   "source": [
    "oof_preds_concat = []\n",
    "sub_preds_concat = []\n",
    "np.random.seed(42) #reproductibility of `np.random.choice`\n",
    "for i in range(num_models):\n",
    "    #each iteraction we use a different seed for LGBM\n",
    "    print(f'model {i+1}')\n",
    "    feats = not_categorical+ np.random.choice([categorical_feature, bin_encoded_cols] ) #choose which type of categorical to use.\n",
    "    oof_preds,sub_preds = train_model_LGBM(X_train, y_train, X_test, kf,feats=feats, seed=i)\n",
    "    oof_preds_concat.append(oof_preds)\n",
    "    sub_preds_concat.append(sub_preds)\n",
    "    \n",
    "oof_preds_concat = np.column_stack(oof_preds_concat)\n",
    "sub_preds_concat = np.column_stack(sub_preds_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple ensemble using the average of all Tree-based models = 1908.0244179521724\n",
      "Using a linear regression = 2281.5258448646937\n"
     ]
    }
   ],
   "source": [
    "#here some insights from the first stacking.\n",
    "print('Simple ensemble using the average of all Tree-based models = {}'.format(np.sqrt(mean_squared_error(y_test, sub_preds_concat.mean(axis=1))) ))\n",
    "lr = LinearRegression()\n",
    "lr.fit(oof_preds_concat , y_train)\n",
    "final_test_preds = lr.predict(sub_preds_concat) \n",
    "print('Using a linear regression = {}'.format(np.sqrt(mean_squared_error(y_test, final_test_preds)) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#main function to train the OLS models\n",
    "def train_model_OLS(data_, y_ ,test_, folds_,formula ):\n",
    "    oof_preds = np.zeros(data_.shape[0])\n",
    "    sub_preds = np.zeros(test_.shape[0])\n",
    "    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_, y_)):\n",
    "        trn_x, trn_y = data_.iloc[trn_idx], y_.iloc[trn_idx]\n",
    "        val_x, val_y = data_.iloc[val_idx], y_.iloc[val_idx]\n",
    "\n",
    "        linear_model = smf.ols(formula=formula, data=pd.concat([trn_x,trn_y], axis=1))\n",
    "        linear_model_fit = linear_model.fit()\n",
    "\n",
    "        oof_preds[val_idx] = linear_model_fit.predict(val_x)\n",
    "        sub_preds += linear_model_fit.predict(test_) / folds_.n_splits\n",
    "\n",
    "        print('Fold %2d MSE : %.6f' %\n",
    "              (n_fold + 1, np.sqrt(mean_squared_error(val_y, oof_preds[val_idx]))))\n",
    "\n",
    "\n",
    "    print('Full MSE score %.6f' % np.sqrt(mean_squared_error(y_, oof_preds)))\n",
    "\n",
    "    return oof_preds, sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1\n",
      "Fold  1 MSE : 3566.139419\n",
      "Fold  2 MSE : 3145.413055\n",
      "Fold  3 MSE : 2261.861477\n",
      "Fold  4 MSE : 2615.892269\n",
      "Fold  5 MSE : 2118.396175\n",
      "Full MSE score 2794.905569\n",
      "model 2\n",
      "Fold  1 MSE : 3991.102978\n",
      "Fold  2 MSE : 3047.178470\n",
      "Fold  3 MSE : 2918.456641\n",
      "Fold  4 MSE : 3374.730076\n",
      "Fold  5 MSE : 2536.560743\n",
      "Full MSE score 3211.060020\n",
      "model 3\n",
      "Fold  1 MSE : 3805.712026\n",
      "Fold  2 MSE : 2892.688511\n",
      "Fold  3 MSE : 3638.810477\n",
      "Fold  4 MSE : 3285.454869\n",
      "Fold  5 MSE : 2960.610097\n",
      "Full MSE score 3336.209178\n",
      "model 4\n",
      "Fold  1 MSE : 3641.179782\n",
      "Fold  2 MSE : 3004.265768\n",
      "Fold  3 MSE : 2986.794304\n",
      "Fold  4 MSE : 3233.472781\n",
      "Fold  5 MSE : 3131.904579\n",
      "Full MSE score 3208.392611\n",
      "model 5\n",
      "Fold  1 MSE : 3226.512915\n",
      "Fold  2 MSE : 2689.909422\n",
      "Fold  3 MSE : 2862.500140\n",
      "Fold  4 MSE : 2958.967418\n",
      "Fold  5 MSE : 2071.054202\n",
      "Full MSE score 2788.715081\n",
      "model 6\n",
      "Fold  1 MSE : 4592.834610\n",
      "Fold  2 MSE : 3878.281111\n",
      "Fold  3 MSE : 4755.526144\n",
      "Fold  4 MSE : 3876.808581\n",
      "Fold  5 MSE : 4297.659622\n",
      "Full MSE score 4295.340406\n",
      "model 7\n",
      "Fold  1 MSE : 4476.886776\n",
      "Fold  2 MSE : 4417.146466\n",
      "Fold  3 MSE : 4100.199911\n",
      "Fold  4 MSE : 3707.695944\n",
      "Fold  5 MSE : 3789.237968\n",
      "Full MSE score 4110.247462\n",
      "model 8\n",
      "Fold  1 MSE : 3595.137971\n",
      "Fold  2 MSE : 2555.929109\n",
      "Fold  3 MSE : 2672.747687\n",
      "Fold  4 MSE : 1974.170853\n",
      "Fold  5 MSE : 2066.170982\n",
      "Full MSE score 2636.959762\n",
      "model 9\n",
      "Fold  1 MSE : 4152.497871\n",
      "Fold  2 MSE : 3376.772938\n",
      "Fold  3 MSE : 3127.799949\n",
      "Fold  4 MSE : 3095.671109\n",
      "Fold  5 MSE : 3067.888582\n",
      "Full MSE score 3388.925686\n",
      "model 10\n",
      "Fold  1 MSE : 3203.378273\n",
      "Fold  2 MSE : 2788.814241\n",
      "Fold  3 MSE : 2152.592948\n",
      "Fold  4 MSE : 2314.131699\n",
      "Fold  5 MSE : 2092.907887\n",
      "Full MSE score 2545.908190\n",
      "model 11\n",
      "Fold  1 MSE : 3747.085959\n",
      "Fold  2 MSE : 3509.179779\n",
      "Fold  3 MSE : 2641.742424\n",
      "Fold  4 MSE : 3652.063619\n",
      "Fold  5 MSE : 1730.595810\n",
      "Full MSE score 3151.708409\n",
      "model 12\n",
      "Fold  1 MSE : 3949.561403\n",
      "Fold  2 MSE : 3067.876676\n",
      "Fold  3 MSE : 3408.683296\n",
      "Fold  4 MSE : 3969.775714\n",
      "Fold  5 MSE : 3126.936501\n",
      "Full MSE score 3526.099133\n",
      "model 13\n",
      "Fold  1 MSE : 4333.325226\n",
      "Fold  2 MSE : 3449.326510\n",
      "Fold  3 MSE : 3674.716584\n",
      "Fold  4 MSE : 3440.161591\n",
      "Fold  5 MSE : 2985.342300\n",
      "Full MSE score 3603.500611\n",
      "model 14\n",
      "Fold  1 MSE : 4859.544573\n",
      "Fold  2 MSE : 5006.984908\n",
      "Fold  3 MSE : 3617.797758\n",
      "Fold  4 MSE : 3495.975307\n",
      "Fold  5 MSE : 3861.422610\n",
      "Full MSE score 4216.775068\n",
      "model 15\n",
      "Fold  1 MSE : 4488.253144\n",
      "Fold  2 MSE : 3784.929062\n",
      "Fold  3 MSE : 3394.292274\n",
      "Fold  4 MSE : 4160.403020\n",
      "Fold  5 MSE : 3378.521568\n",
      "Full MSE score 3865.609541\n"
     ]
    }
   ],
   "source": [
    "#here we removed the columns with NaN to avoid numerical problems. \n",
    "categorical_options = ['make', 'fuel_type', 'aspiration', 'body_style', 'drive_wheels', 'engine_location', 'engine_type',\n",
    "                       'num_of_cylinders', 'fuel_system']\n",
    "\n",
    "numerical_options = ['symboling','wheel_base', 'length','width','height','curb_weight','engine_size','compression_ratio',\n",
    "                     'city_mpg','highway_mpg']\n",
    "\n",
    "oof_preds_concat_ols = []\n",
    "sub_preds_concat_ols = []\n",
    "num_models = 15\n",
    "np.random.seed(42) #reproductibility of `np.random.choice`\n",
    "for i in range(num_models):\n",
    "    print(f'model {i+1}')\n",
    "    #for each model we will use a set of features randomly chosen\n",
    "    feat1, feat2, feat3, feat4 = np.random.choice(numerical_options, 4, replace=False )\n",
    "    feat5, feat6, feat7 = np.random.choice(categorical_options, 3, replace=False )\n",
    "    formula =f\"price ~  C({feat5}) + C({feat6}) + C({feat7}) + {feat1} + {feat2} + {feat3} + {feat4}\"\n",
    "    oof_preds,sub_preds = train_model_OLS(X_train, y_train, X_test, kf,formula=formula)\n",
    "    oof_preds_concat_ols.append(oof_preds)\n",
    "    sub_preds_concat_ols.append(sub_preds)\n",
    "    \n",
    "oof_preds_concat_ols = np.column_stack(oof_preds_concat_ols)\n",
    "sub_preds_concat_ols = np.column_stack(sub_preds_concat_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple ensemble using the average of all OLS models = 2456.8550811469754\n",
      "Using a linear regression for each model = 2265.701447161632\n"
     ]
    }
   ],
   "source": [
    "print('Simple ensemble using the average of all OLS models = {}'.format(np.sqrt(mean_squared_error(y_test, sub_preds_concat_ols.mean(axis=1))) ))\n",
    "lr = LinearRegression()\n",
    "lr.fit(oof_preds_concat_ols , y_train)\n",
    "final_test_preds = lr.predict(sub_preds_concat_ols) \n",
    "print('Using a linear regression for each model = {}'.format(np.sqrt(mean_squared_error(y_test, final_test_preds)) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Puting everthing together.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds =  np.concatenate([oof_preds_concat, oof_preds_concat_ols], axis=1)\n",
    "sub_preds =  np.concatenate([sub_preds_concat, sub_preds_concat_ols], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple ensemble using the average of OLS+tree models = 2078.0432675879183\n",
      "Using a linear regression = 2611.756699187603\n"
     ]
    }
   ],
   "source": [
    "print('Simple ensemble using the average of OLS+tree models = {}'.format(np.sqrt(mean_squared_error(y_test, sub_preds.mean(axis=1))) ))\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(oof_preds , y_train)\n",
    "final_test_preds = lr.predict(sub_preds) \n",
    "print('Using a linear regression = {}'.format(np.sqrt(mean_squared_error(y_test, final_test_preds)) ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final conclusion\n",
    "\n",
    "\n",
    "We noticed a lot of variance between the folds, we could use more folds. \n",
    "The final toot mean square error was good, we have a mean price value of `$13207.12` we got RMSE of `$2078.04`\n",
    "\n",
    "we could improve the stacking using other than LinearRegression or mean value, e.g., xgboost, lightgbm, NN, SVR etc. Also, since we noticed that the performance of the lightgbm is better than the OLS, we could weight its models higher than the OLS ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
